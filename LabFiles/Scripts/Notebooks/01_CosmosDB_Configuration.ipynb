{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Load the IoTDeviceInfo dataset from ADLS Gen2 to a dataframe\n",
        ">he Synapse workspace is attached to an ADLS Gen2 storage account and the files placed on the default storage account can be accessed using the relative path as below.\n",
        "&nbsp;\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfDeviceInfo = (spark\n",
        "                .read\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoIoT/IoTDeviceInfo.csv\", header=True)\n",
        "              )\n",
        "\n",
        "dfSignals = (spark\n",
        "                .read\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoIoT/IoTSignals.csv\", header=True)\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Write the dataframe to the Azure Cosmos DB collection\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfDeviceInfo.write\\\n",
        "            .format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemoIoT\")\\\n",
        "            .option(\"spark.cosmos.container\", \"IoTDeviceInfo\")\\\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "            .mode('append')\\\n",
        "            .save()\n",
        "\n",
        "dfSignals.write\\\n",
        "            .format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemoIoT\")\\\n",
        "            .option(\"spark.cosmos.container\", \"IoTSignals\")\\\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "            .mode('append')\\\n",
        "            .save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Simulate streaming data generation using Rate streaming source\n",
        "* The Rate streaming source is used to simplify the solution here and can be replaced with any supported streaming sources such as [Azure Event Hubs](https://azure.microsoft.com/en-us/services/event-hubs/) and [Apache Kafka](https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction).\n",
        "\n",
        "\n",
        ">The Rate streaming source generates data at the specified number of rows per second and each output row contains a timestamp and value.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfStream = (spark\n",
        "                .readStream\n",
        "                .format(\"rate\")\n",
        "                .option(\"rowsPerSecond\", 10)\n",
        "                .load()\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Format the stream dataframe as per the IoTSignals schema\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "import uuid\n",
        "\n",
        "numberOfDevices = 10\n",
        "generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\n",
        "              \n",
        "dfIoTSignals = (dfStream\n",
        "                    .withColumn(\"id\", generate_uuid())\n",
        "                    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\n",
        "                    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)+1))\n",
        "                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\n",
        "                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\n",
        "                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\n",
        "                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.9 THEN value * 2 WHEN rand() < 0.1 THEN value div 2 ELSE value END\"))\n",
        "                    .drop(\"timestamp\")\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Stream writes to the Azure Cosmos DB Collection\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "streamQuery = dfIoTSignals\\\n",
        "        .writeStream\\\n",
        "        .format(\"cosmos.oltp\")\\\n",
        "        .outputMode(\"append\")\\\n",
        "        .option(\"checkpointLocation\", \"/writeCheckpointDir\")\\\n",
        "        .option(\"spark.synapse.linkedService\", \"CosmosDemoIoT\")\\\n",
        "        .option(\"spark.cosmos.container\", \"IoTStreamingSignals\")\\\n",
        "        .option(\"spark.cosmos.connection.mode\", \"gateway\")\\\n",
        "        .start()\n",
        "\n",
        "streamQuery.awaitTermination()"
      ]
    }
  ]
}